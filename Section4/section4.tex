\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}
\usepackage{natbib}
\usepackage{amsmath,amsfonts,graphicx}
\usepackage{float}

%
% The following commands set up the lecnum (chap number)
% counter and make various numbering schemes work relative
% to the chap number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\chap}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf SDS 383D: Modeling II
	\hfill Spring 2017} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Section #1: #2  \hfill} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Section #1: #2}{Section #1: #2}

   
}
%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
%\renewcommand{\cite}[1]{[#1]}
%\def\beginrefs{\begin{list}%
%        {[\arabic{equation}]}{\usecounter{equation}
%         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
%         \setlength{\labelwidth}{1.6truecm}}}
%\def\endrefs{\end{list}}
%\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{exercise}{Exercise}[lecnum]
\newtheorem{example}{Example}[lecnum]
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\newcommand\E{\mathbb{E}}
\newcommand\Prob{\mathbf{P}}
\newcommand\Q{\mathbf{Q}}
\newcommand\cov{\mbox{cov}}
\begin{document}
\chap{4}{Gaussian processes}
\maketitle

\section{Non-linear functions}

\subsection{Regression view}

So far, we've assumed our latent function is a linear function of our data -- which is obviously limiting. One way of circumventing this is to project our inputs into some high-dimensional space using a set of basis functions $\phi:\mathbb{R}^d\rightarrow \mathbb{R}^N$, and then performing linear regression in that space, so that

$$y_i = \phi(x)^T\beta + \epsilon_i$$

For example, we could project $x$ into the space of powers of $x$, i.e. $\phi(x) = (1,x,x^2,x^3\dots)$ to obtain polynomial regression.


\begin{exercise}
  Let $\mathbf{y}$ and $\mathbf{X}$ be set of observations and corresponding covariates, and $y_*$ be the unknown value we wish to predict at covariate $\mathbf{x}_*$.  Assume that

  $$\begin{aligned}
    \beta \sim& \mbox{N}\left(0,\Sigma\right)\\
    \begin{bmatrix}f_* \\ \mathbf{f}\end{bmatrix} =& \begin{bmatrix}\boldsymbol{\phi}_*^T\\ \boldsymbol{\Phi}^T\end{bmatrix}^T\beta\\
      \begin{bmatrix}y_* \\ \mathbf{y}\end{bmatrix} \sim& \mbox{N}\left(\begin{bmatrix}f_* \\ \mathbf{f}\end{bmatrix},\sigma^2\mathbf{I}\right)
      \end{aligned}$$
  where $\boldsymbol{\phi}:=\phi(\mathbf{x})$ and $\boldsymbol{\Phi}:=\phi(\mathbf{X})$.
  
  What is the predictive distribution $p(f_*|\mathbf{y},\mathbf{x}_*,\mathbf{X})$? Note: this is very similar to questions we did in Section 1.
 
\end{exercise}

Note that, in the solution to Exercise 1, we only ever see $\phi$ or $\Phi$ in a form such as $\Phi^T\Sigma\Phi$. We will define $k(\mathbf{x},\mathbf{x}') = \phi(\mathbf{x})^T\Sigma\phi(\mathbf{x}')$. Since $\Sigma$ is positive definite, we can write:

$$k(\mathbf{x},\mathbf{x}') = \psi(\mathbf{x})^T\psi(\mathbf{x})$$
where $\psi(\mathbf{x}) = \phi(\mathbf{x})\Sigma^{1/2}$

If (as here) we only ever access $\psi$ via this inner product, we can choose to work instead with $k(\cdot,\cdot)$. This may be very convenient if the dimensionality of $\psi(x)$ is very high (or even infinite... see later). $k(\cdot,\cdot)$ is often refered to as the kernel, and this replacement is referred to as the kernel trick.

\textbf{Begin Solution:}
\begin{equation*}
\begin{split}
p(f^*|x_*,X,y) &= \int p(f^*|x_*,\beta) p(\beta|X,y) d\beta\\
f^* = \phi(x)\beta &\Rightarrow f^*|X_*,\beta \sim N(0,\phi(x)^T\Sigma\phi(x))\\
p(\beta|x,y)&\propto p(y|X,\beta)p(\beta)\\
\Rightarrow f^*|x_*,X,y &\sim N(\frac{1}{\sigma^2}X_*^TA^{-1}Xy,X_*^TA^{-1}X_*)\\
\text{where}\\
A &= \sigma^{-2}XX^T+\Sigma^{-1}\\
\text{Replace x with} \phi(x),X with \phi(X)\\
f^*|x_*,X,y &\sim N(\frac{1}{\sigma^2}\phi^T(x_*)A^{-1}\phi(X)y,\phi^T(x_*)A^{-1}\phi(x_*))\\
\text{where}\\
A &= \sigma^{-2}\phi(x)\phi(x)^T+\Sigma^{-1}
\end{split}
\end{equation*}
\textbf{Solution End}

\begin{exercise}

  Let's look at a concrete example, using the old faithful dataset on $R$
  \begin{itemize}
  \item \texttt{data("faithful",package="datasets")} in R
  \item or available as \texttt{faithful.csv} on github if you're not using R.
  \end{itemize}

  Let $\phi(x) = (1,x,x^2,x^3)$. Using appropriate priors on $\beta$ and $\sigma^2$, obtain a posterior distribution over $f:=\phi(x)^T\beta$. Plot the function (with a 95\% credible interval) by evaluating this on a grid of values.
\end{exercise}
  
\textbf{Begin Solution:}
For details please see the code on github repository.
\begin{figure}[H]
\begin{center}
\includegraphics[width=\textwidth]{./Exercise42/Exercise42.png}
\end{center}
\caption{Exercise 4.2}
\end{figure} 
\textbf{Solution End}

\subsection{Function space view}

Look back at the plot from Exercise 2. We specified a prior distribution over regression parameters, which we can use to obtain a posterior distribution over those regression parameters. But, what we calculated (and plotted) was a posterior distribution over \textit{functions}. Similarly, we can think of our prior on $\beta$ as specifying a prior distribution on the space of cubic functions. Evaluated at a finite number of input locations -- as you did in Exercise 2 -- this posterior distribution is multivariate Gaussian. This is in fact the definition of a Gaussian process: A distribution over functions, such that the marginal distribution evaluated at any finite set of points is multivariate Gaussian.

A priori, the  covariance of $f$ is given by $$\cov(x,x') = E[(f(x)-m(x))(f(x^T)-m(x^T))] = k(x,x')$$. For this reason, our kernel $k$ is often referred to as the covariance function (note, it is a function since we can evaluate it for any pairs $x,x'$). In the above example, where $\beta$ had zero mean, the mean of $f$ is zero; more generally, we will assume some mean function $m(x)$.

Rather than putting a prior distribution over $\beta$, we can specify a covariance function -- remember that our covariance function can be written in terms of the prior covariance of $\beta$.  For example, we might let

$$k(x,x') = \alpha^2\exp\left\{-\frac{1}{2\ell^2}|x-x'|^2\right\}$$
 -- this is known as a squared exponential covariance function, for obvious reasons. This prior encodes the following assumptions:
\begin{itemize}
\item The covariance between two datapoints decreases monotonically as the distance between them increases.
\item The covariance function is stationary -- it only depends on the distance between $x$ and $x'$, not their locations.
\item Even more than being stationary, it is isotropic: It depends only on $|x-x'|$.
\end{itemize}

\begin{exercise}
  Let's explore the resulting distribution over functions. Write some code to sample from a Gaussian process prior with squared exponential covariance function, evaluated on a grid of 200 inputs between 0 and 100. For $\ell=1$, sample 5 functions and plot them on the same plot. Repeat for $\ell=0.1$ and $\ell=10$. Why do we call $\ell$ the lengthscale of the kernel?
\end{exercise}

\textbf{Begin Solution:}
For details please see the code on github repository.
\begin{figure}[H]
\begin{center}
\includegraphics[width=\textwidth]{./Exercise43/Figure43.png}
\end{center}
\caption{Exercise 4.3}
\end{figure} 
\textbf{Solution End}

\begin{exercise}
  Let $\mathbf{f}_*:=f(\mathbf{X}_*)$ be the function $f$ evaluated at test covariate locations $\mathbf{X}_*$. Derive the posterior distribution $p(\mathbf{f}_*|\mathbf{X}_*,\mathbf{X},\mathbf{y})$, where $\mathbf{y}$ and $\mathbf{X}$ comprise our training set. (You can start from the answer to Exercise 1 if you'd like).

\end{exercise}

\textbf{Begin Solution:}
According to Equation (2.22),
\begin{equation*}
\begin{split}
f^*|X,y,x_*&\sim N(\bar{f}_*,cov(f^*))\\
cov(f^*)&=K(x_*,x_*)-K(X_*,X)\left[ K(x,x) + \sigma_n^2I\right]^{-1}K(X,X_*)
\end{split}
\end{equation*}
\textbf{End Solution}

\begin{exercise}
Return to the faithful dataset. Evaluate the posterior predictive distribution 
$p(\mathbf{f}_*|\mathbf{X}_*,\mathbf{X},\mathbf{y})$, for some reasonable choices of parameters (perhaps explore a few length scales if you're not sure what to pick), and plot the posterior mean plus a 95\% credible interval on a grid of 200 inputs between 0 and 100, overlaying the actual data.
\end{exercise}

\textbf{Begin Solution:}
\begin{figure}[H]
\begin{center}
\includegraphics[width=\textwidth]{./Exercise45/exercise44.png}
\end{center}
\caption{Exercise 4.5}
\end{figure} 
\textbf{End Solution}

  
\section{Parameter Estimation}


As we saw in the previous section, the choice of hyperparameters (for the squared exponential case, the length scale $\ell$) effects the properties of the resulting function. Rather than pick a specific value for the hyperparameter, we can specify the model in a hierarchical manner---just like we did in the linear case.

For example, in the squared exponential setting, we could specify our model as

$$\begin{aligned}
  \ell^2 \sim& \mbox{Inv-Gamma}(a_\ell, b_\ell)\\
  \alpha^2 \sim& \mbox{Inv-Gamma}(a_\alpha, b_\alpha)\\
  \sigma^2 \sim& \mbox{Inv-Gamma}(a_\sigma, b_\sigma)\\
  k(x,x') =& \alpha^2\exp\left\{-\frac{1}{2\ell^2}|x-x'|^2\right\}+\sigma^2\delta_{x-x'}\\
  y|X \sim& N(0,\tilde{K})
\end{aligned}
$$
where $K$ is the covariance function evaluated at the input locations $X$. Note that we have integrated out $f$ and placed our prior directly on $y$, incorporating the Gaussian likelihood into the covariance. We can then infer the posterior distribution over $\ell$ using Bayes' Law:

 $$p(\ell|y,X) = \frac{p(y|X,\ell)p(\ell)}{\int_0^\infty p(y|X,\ell)p(\ell)d\ell}$$

 Unfortunately, we typically do not have an analytical form for this posterior, so we must resort to either optimization, or MCMC-based inference.

 \subsection{Optimization}
 In practice, a common approach is to find the ML estimate for the hyperparameters. Let's assume a generic setting, where the log likelihood is parametrized by some vector of parameters $\theta$. The log likelihood is given by

 $$log p(y|X,\theta) = -\frac{1}{2}y^TK^{-1}y - \frac{1}{2}\log|K|-\frac{n}{2}\log 2 \pi$$

 Taking partial derivatives, we see that

 $$\begin{aligned}\frac{\partial}{d\partial \theta_j} \log p(y|X,\theta) =& \frac{1}{2}y^TK^{-1}\frac{\partial K}{\partial \theta_j} K^{-1}y - \frac{1}{2}\mbox{tr}\left(K^{-1}\frac{\partial K}{\partial \theta_j}\right)\\
   =& \frac{1}{2}\mbox{tr}\left((\alpha \alpha^T - K^{-1})\frac{\delta K}{\delta \theta_j}\right)\end{aligned}
 $$
 where $\alpha = K^{-1}y$. We can use these partial derivatives to find the ML estimate of $\theta$, using a gradient-based optimization method


 \begin{exercise}
   Calculate the appropriate derivatives for the one-dimensional, squared exponential case used for the \texttt{faithful} dataset. Use these gradient to find the optimizing value of $\ell^2$, $\alpha^2$ and $\sigma^2$. Plot the resulting fit.
 \end{exercise}

\textbf{Begin Solution:}
\begin{figure}[H]
\begin{center}
\includegraphics[width=\textwidth]{./Exercise46/exercise46.png}
\end{center}
\caption{Exercise 4.6}
\end{figure} 
\textbf{End Solution}

 \begin{exercise}
   Repeat the previous exercise, but this time only use the first 10 data points from the faithful dataset. Repeat the optimization several times, using different initializations/random seeds. You will likely see widely different results -- sometimes $\ell$ is big, sometimes $\sigma^2$ is big. Why is this? Discuss why this is a problem here, but wasn't in the previous setting. You may find it helpful to look at the corresponding scatter plot, or plot the log likelihood for certain values of $\sigma^2$ and $\ell$.
 \end{exercise}
\textbf{Begin Solution:}
\begin{figure}[H]
\begin{center}
\includegraphics[width=\textwidth]{./Exercise47/exercise471.png}
\includegraphics[width=0.75\textwidth]{./Exercise47/exercise472.png}
\end{center}
\caption{Exercise 4.7}
\end{figure}
The widely different result is due to different local minimum in the optimization.\\  
\textbf{End Solution}
 
 \subsection{MCMC}
 Optimization is typically pretty quick, which is why it is commonly used in practice. However, we have no guarantee that our optimization surface is convex. An alternative approach is to sample from the posterior distribution over our hyperparameters.


 \begin{exercise}
   Since the posterior is non-conjugate, we can't use a Gibbs sampler. We won't go into the details of appropriate sampling methods since this isn't an MCMC course, but we will explore using black-box samplers. In the R folder, there are three files: \texttt{faithful\_data.R}, \texttt{gp\_regression.stan} and \texttt{run\_gp\_regression.R}. Use these to sample from the model and produce 95\% credible intervals for $\alpha$, $\ell$ and $\sigma$, and 95\% predictive intervals for $t$. Go through the code and make sure you understand what is going on.

 \end{exercise}

\textbf{Begin Solution:}
The 95\% confidence interval for $\alpha$ is [1.518168, 1.895251].\\
The 95\% confidence interval for $\rho$ is [11.86324,17.87650].\\
The 95\% confidence interval for $\sigma$ is [0.3058305,0.3519437].\\
The 95\% confidence interval for $t$ is [-2.361714,6.372179].\\
\textbf{End Solution}

 \begin{exercise}
   Let's now look at a dataset with multiple predictors. Download the dataset weather.csv -- this contains latitude and longitude data for 147 weather stations, plus a response ``temperature'', which is the difference between the forecasted and actual temperature for each station.

   How should we extend our kernel to multiple dimensions? (There is more than one option here). Should we use the same lengthscale for latitude and longitude?  Construct an appropriate parametrized kernel, and learn the parameters either via optimization or using MCMC by editing the Stan code (Note: If you go for the stan code, you will need to implement your new kernel).

   Using an appropriate visualization tool, plot the mean function (try imshow or contourf in matlab or matplotlib (for python), or image or filled.contour for R).
 \end{exercise}

\textbf{Begin Solution:}
One way to extend the kernel is by making $\phi(x)$ vectors. For example $\phi(x)=[x_{lat},x_{long}]$. The length scales of latitude and longitude need to be different as the data covers different areas. Also, latitude and longitude value ranges are different.\\
\begin{figure}[H]
\begin{center}
\includegraphics[width=\textwidth]{./Exercise49/Exercise49.png}
\end{center}
\caption{Exercise 4.9}
\end{figure}
\textbf{End Solution}

 \section{Beyond regression: non-conjugate likelihoods}

 So far, we've focused on Gaussian processes in a regression context. We can however use them as the basis of a non-Gaussian regression... using exactly the same techniques as we used for the regression setting! For example, in Section 3, we dealt with non-Gaussian data by transforming our regression output:

 $$y_i|\beta,x_i \sim f(g^{-1}(x_i^T\beta))$$

 where $f$ is an appropriate likelihood model (e.g.\ Bernoulli for binary data, Poisson for count data) and $g^{-1}$ was a function that maps the real-valued $x_i^T\beta$ to an appropriate space for that likelihood.


 We can do exactly the same here, by letting

 $$\begin{aligned}f \sim& \mbox{GP}(0, k)\\
   y_i \sim&f(g^{-1}(f(x_i)))\end{aligned}
 $$


 Let's start by considering a binary example. In the GLM setting, we looked at both probit and logit regression. We can use the same approaches here!

 \begin{exercise}
   Describe (including pseudo-code) how we could implement probit Gaussian process regression, using an auxiliary variable method analogous to that used in Exercise 3.1.
 \end{exercise}
\textbf{Begin Solution}
First, add a latent variable that follows a Gaussian distribution. Next find the conditional distribution of this variable. Lastly convert this continuous latent variable to binary using probit function.\\
\textbf{End Solution}

 For the logit case, we can again use a Laplace approximation to approximate our posterior. In the GLM setting, we used the Laplace approximation to approximate the posterior over $\beta$. Here, we will work directly with our function $f$ evaluated at our training locations, and approximate $p(f|X,y,\theta)$, where $\theta$ are the parameters of our covariance function.

 Let $P^*(f)\propto p(f|X,y,\theta)$ be our unnormalized posterior, so that $\log P^*(f) = \log p(y|f) + log p(f|X) = \log p(y|f) -\frac{1}{2}f^TK^{-1}f - \frac{1}{2}\log|K| + \mbox{const}$.

 \begin{exercise}
   Derive the Hessian of $\log P^*(f)$, when $y_i \sim \mbox{Bernoulli}\left(\frac{1}{1+e^{-f_i}}\right)$
 \end{exercise}
\textbf{Begin Solution}
\begin{equation*}
\begin{split}
\frac{\partial P^*(f)}{\partial f} &= \sum_i\left\{{y_i}\frac{1}{\sigma(f_i)}\cdot \sigma^{'}(f_i)+(1-y_i)\frac{-\sigma(-f_i)}{1-\sigma(-f_i)}-\frac{1}{2}f_i^TK^{-1}\right\}\\
\frac{\partial^2 P^*(f)}{\partial f^2} &=\sum_i\left\{y_i\frac{\sigma(f)\sigma^{''}(f)-\sigma^{'}(f)^2}{\sigma(f_i)^2}+(1-y_i)\frac{(\sigma(f)-1)(\sigma^{''}(f)-\sigma^{'}(f))}{(1-\sigma(f))^2}-\frac{1}{2}K^{-1} \right\}
\end{split}
\end{equation*}
where
\begin{equation*}
\sigma(f_i) = \frac{1}{1+\exp(-f_i)} \hspace{1cm} \sigma^{'}(f_i) = \frac{\exp(-f_i)}{(1+\exp(-f_i))^2} \hspace{1cm} \sigma^{''}(f_i) = \frac{2\exp(-2f_i)}{(1+\exp(-f_i))^3}-\frac{\exp(-f_i)}{(\exp(-f_i)+1)^2}
\end{equation*}
\textbf{End Solution}

 \begin{exercise}
   The dataset iris.csv contains details of 150 flowers from three species. Pick two of them (your choice) as your regression dataset. Find the MAP of $f$, for some reasonable choice of hyperparameters and squared exponential kernel. Visualize the corresponding class probabilities on a series of 2d plots.
 \end{exercise}
\textbf{Begin Solution:}
For details please see the code on github repository.
\begin{figure}[H]
\begin{center}
\includegraphics[width=\textwidth]{./Exercise4_12/Probplot.png}
\end{center}
\caption{Exercise 4.12}
\end{figure}
\textbf{End Solution}
 \begin{exercise}
   Evaluate the Hessian using the same dataset, and visualize uncertainty in your plots from the previous exercise (e.g.\ by creating a contour plot of the marginal standard deviations).
   \end{exercise}
\textbf{Begin Solution:}
\\See solution for 4.12\\
\textbf{End Solution}

 \begin{exercise}
   In a multi-class setting, an appropriate likelihood is the multinomial, which is parametrized by a simplex-valued vector $\pi = (\pi_1,\dots, \pi_K)$. We can map a real-valued vector $y$ to the simplex using the softmax transformation:

   $$\pi_i = \frac{e^{y_i}}{\sum_{j=1}^K e^{y_k}}$$

   To use this transformation, we will have to have one Gaussian process for each class.\footnote{Technically, we could use $K-1$ GPs plus a constant reference value for the third class, but let's use $K$ for now.} Practically, we can think of this as using a single Gaussian process, but with a block-diagonal covariance matrix with K $N\times N$ blocks. Using a Laplace approximation to the posterior distribution, repeat the previous three exercises using all three types of iris.
   \end{exercise}
   
\textbf{Begin Solution:} 
For details please see code on github repository.
\begin{figure}[H]
\begin{center}
\includegraphics[width=\textwidth]{./Exercise4_14/class.png}
\includegraphics[width=\textwidth]{./Exercise4_14/prob.png}
\end{center}
\caption{Exercise 4.14}
\end{figure}

\textbf{End Solution} 

  \bibliographystyle{apalike}
  \bibliography{course}

\end{document}
